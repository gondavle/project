#!/usr/bin/env python
# coding: utf-8

# In[1]:


import warnings
warnings.filterwarnings("ignore")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report,accuracy_score, confusion_matrix

from sklearn.svm import LinearSVC
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score, GridSearchCV


# In[2]:


df=pd.read_csv("train.csv")
df


# In[3]:


df.head()


# In[4]:


df.tail()


# In[5]:


df.info()


# In[6]:


df.describe()


# In[7]:


df.shape


# In[8]:


df.columns


# In[9]:


df.isnull().sum()


# # Class Distribution

# In[10]:


df["Activity"].unique()


# # Now Visualize the class Distribution

# In[11]:


plt.figure(figsize=(12,6))
sns.countplot(x="Activity",data=df)
plt.show()


# # Now Subject Part

# In[12]:


df["subject"].unique()


# # Drop the Table

# In[13]:


x=pd.DataFrame(df.drop(["Activity","subject"],axis=1))


# In[14]:


x


# In[15]:


num_cols=x._get_numeric_data().columns
print("Number of numeric feature:",num_cols.size)


# In[16]:


y=df.Activity.values.astype(object)


# In[17]:


y[5]


# # Transforming Non numerical Labels into numerical labels(Encoding)

# In[18]:


le=LabelEncoder()
y=le.fit_transform(y)
y.shape


# In[19]:


y[5]


# # Feature Scaling

# In[20]:


scaler=StandardScaler()


# In[21]:


x=scaler.fit_transform(x)


# In[22]:


x


# # split tarining and testing

# In[23]:



x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=1)


# # Now Train The Model

# # firstly SVM

# In[24]:


lin_svc = LinearSVC(random_state = 1)
lin_svc.fit(x_train,y_train)


# In[25]:


y_pred = lin_svc.predict(x_test)


# In[26]:


print(classification_report(y_test,y_pred))


# # Now Random Forest Classifier

# In[27]:


rand_clf=RandomForestClassifier(random_state=5)


# In[28]:


rand_clf.fit(x_train,y_train)


# In[29]:


rand_clf.score(x_test,y_test)


# # Now, manually setting the hyperparameters, and using GridSearchCV for Hyperparameter Tuning:

# In[30]:


grid_param={
    'n_estimators':[90,100,115,130],
    'criterion':['gini','entropy'],
    'max_depth':range(2,20,1),
    'min_samples_leaf':range(1,10,1),
    'min_samples_split':range(2,10,1),
    'max_features':['auto','log2']
}


# In[31]:


grid_search=GridSearchCV(estimator=rand_clf,param_grid=grid_param,cv=5,n_jobs=-1,verbose=3)


# In[ ]:


grid_search.fit(x_train, y_train)


# In[ ]:


grid_search.best_params_


# In[ ]:




